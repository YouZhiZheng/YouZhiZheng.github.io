# 西瓜书学习笔记


## 第2章 模型评估与选择

### 经验误差与过拟合

**错误率：** 分类错误的样本占样本总数的比例。设m个样本中有a个样本分类错误，则错误率 $E = a / m$。

**精度：** 分类正确的样本占样本总数的比例。即 &#34;精度 = 1 - 错误率&#34;（精度一般用百分比表示）。

&lt;br&gt;一般来说，将学习器的实际预测输出与样本的真实输出之间的差异称为 &#34;误差&#34;。

**训练误差：** 学习器在训练集上的误差（也叫经验误差）。

**泛化误差：** 学习器在新样本上的误差。

&lt;br&gt;我们希望得到的学习器是能在新样本上有很好表现的学习器。为了能达到这个目的，应该从训练样本中尽可能学出适用于所有潜在样本的普遍规律，这样才能在遇到新样本时做出正确判断。然而，当学习器把训练样本学得太好时就会出现「过拟合」现象。与过拟合相对的是「欠拟合」。

**过拟合：** 学习器把训练样本自身的一些特点当作了所有潜在样本的特点，导致泛化性能下降。

**欠拟合：** 学习器对训练样本的一般性质尚未学好，没有很好地学习到训练样本中的模式和规律。

![图1 过拟合、欠拟合的直观类比](/PostsImgs/Machine_Learing_imgs/picture1.png)

导致过拟合的因素有很多，其中最常见的情况是学习能力过于强大（学习能力由学习算法和数据内涵共同决定），以至于学习器把训练样本所包含的不太一般的特性都学到了。而欠拟合则通常是由于学习能力低下而造成的。过拟合是机器学习面临的关键障碍，是无法避免的，我们所能做的只是 &#34;缓解&#34; 过拟合。

&lt;br&gt;在现实任务中，我们有许多学习算法可以选择，各类算法都必然带有一些针对过拟合的措施，甚至同于同一个算法来说，使用不同的参数配置时，也会产生不同的模型。那么该选择哪一个学习算法、使用哪一种参数配置呢？这就是机器学习中的「模型选择」问题。理想的解决方案为对候选模型的泛化误差进行评估，然后选择泛化误差最小的模型。然而，我们是无法直接获取泛化误差的，而训练误差又由于过拟合现象的存在不适合作为标准，那么在现实中如何进行模型的评估与选择呢？

### 评估方法

我们通常会将数据集`D`划分为训练集`S`和测试集`T`，使用`T`来测试学习器对新样本的判别能力，然后以`T`上的测试误差作为泛化误差的近似。需要注意的是，`T`与`S`应该尽可能的 **「互斥」**。

#### 1. 留出法

该方法直接将数据集`D`划分为两个互斥的集合，其中一个作为训练集`S`，另一个作为测试集`T`，即$D=S \cup T, S \cap T=\varnothing$。使用测试误差来近似泛化误差。

需要注意的是，S 和 T 的划分要尽可能保持数据分布的一致性（即在不同数据集或数据样本中，数据的分布特性保持相同或相似的性质），避免因数据划分过程引入额外的偏差而对最终结果产生影响。

单次使用留出法得到的结果往往都是不够稳定和可靠的，因此，在使用留出法时，一般采用多若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。此外，我们希望评估的是用数据集D训练出来的模型的性能，但留出法需将D划分为S和T，这就会导致一个问题：若训练集包含绝大多数样本，则训练出的模型会更接近用D训练出的模型，但由于T比较小，评估结果可能不够稳定准确；若T多包含一些样本，则S与D的差别就会变大，导致被评估的模型与用D训练出的模型相比可能有较大差异，从而降低了评估结果的保真性。这个问题没有完美的解决方案，常见做法是将大约 **2 / 3 ~ 4 / 5** 的样本作为训练，剩下的作为测试。

#### 2. 交叉验证法

该方法将数据集D划分为 k 个大小相似的互斥子集，即$D=D_{1} \cup D_{2} \cup \ldots \cup D_{k}, D_{i} \cap D_{j}=\varnothing(i \neq j) \text {. }$每个子集都尽可能保持数据分布的一致性。每次用`k-1`个子集的并集作为训练集，剩下的那个子集作为测试集；这样就可以得到`k`组训练/测试集，进行`k`次训练和测试。所以，交叉验证法又被称为`k`折交叉验证法。常用的k值有5、10、20等。

与留出法相似，将D划分成多个子集有多种划分方式，为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分法重复`p`次，最终的评估结果为这p次的均值。

假设数据集D中包含m个样本，若令 k = m，则得到了交叉验证法的特例：**留一法**。显然，留一法不受随机样本划分方式的影响，这就使得在绝大多数情况下，通过留一法训练出来的模型跟用D训练出来的模型很相似。因此，留一法的评估结果往往被认为是比较准确的。但其缺陷也很明显，在数据集较大时，留一法的计算开销十分大。

#### 3. 自助法

前面两种方法必然存在因训练样本规模不同而导致的估计偏差，留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。有没有什么方法可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？

自助法就是一个比较好的解决方案，该方法的思路为：对于给定包含m个样例的数据集D，对其进行m次采样，每次采样随机选择一个样例将其 **复制** 到数据集 $D&#39;$。这样我们就可以得到一个包含m个样例的数据集 $D&#39;$，显然，D中的一部分样例会在 $D&#39;$中重复出现，另一部分样例不会在 $D&#39;$中出现。样本始终不被采到的概率为 $(1 - \frac{1}{m})^m$，取极限得到：  
&lt;div style=&#34;text-align: center&#34;&gt; $\lim_{m \to \infty} (1 - \frac{1}{m})^m = \frac{1}{e}$ &lt;/div&gt;

即通过自助采样，初始数据集D中约有 **36.8%** 的样本不会出现在数据集 $D&#39;$。于是将 $D&#39;$ 作为训练集，D - $D&#39;$（即没有被采样的样例） 作为测试集。

自助法在数据集较小、难以有效划分训练/测试集时很有用，但其产生的数据集改变了初始数据集的分布，这会引用估计偏差。因此，在初始数据量足够的情况下，留出法和交叉验证法更常用一些。

#### 交付的模型

在训练模型时我们只使用了一部分数据训练模型。因此，在模型选择完成后，学习算法和参数配置都已经确定，此时应该用完整的数据集D重新训练模型。这个模型在训练过程中使用了所有m个样本，这才是我们最终会使用的模型。

### 性能度量

对学习器的泛化能力进行评估，不仅需要有效可行实验评估方法，还需要有衡量模型泛化能力的 **评价标准**，这就是性能度量。在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果，这说明模型的好坏是相对的，还决定于任务的需求。

在预测任务中，给定样例集 $D = { (x_1, y_1),(x_2, y_2), ... , (x_m, y_m) \}$ ，其中 $y_i$ 是示例 $x_i$ 的真是标记。要评估学习器 $f$ 的性能，就是要把学习器的预测结果与 $f(x)$ 与真实标记 $y$ 做比较。

**回归任务**最常用的性能度量是 **「均方误差」**。

&lt;br&gt;下面主要介绍 **分类任务** 中常用的性能度量。

#### 1. 错误率和精度

前面介绍过了，这里就不多赘述了。

#### 2. 查准率、查全率与F1

错误率和精度虽然常用，但并不能满足所有任务需求。例如在信息检索中，我们经常会关心“检索出的信息中有多少比例是用户感兴趣的”“用户感兴趣的信息中有多少被检索出来了”。「查准率」和「查全率」就是适用于此类需求的性能度量。

对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例(true positive)、假正例(false positive)、真反例(true negative)、假反例(false negative)四种情况，令TP、FP、TN、FN分别表示其对应的样例数，则显然有 TP &#43; FP &#43; TN &#43; FN = 样例总数。分类结果的混淆矩阵如下表所示：

![图2 分类结果矩阵](/PostsImgs/Machine_Learing_imgs/picture2.png)

**查准率P**和**查全率R**分别定义为：

&lt;div style=&#34;text-align: center&#34;&gt;$P = \frac{TP}{TP&#43;FP} $&lt;/div&gt;

&lt;div style=&#34;text-align: center&#34;&gt;$R = \frac{TP}{TP&#43;FN} $&lt;/div&gt;

查准率衡量的是模型在所有被预测为正的样本中，实际为正的样本比例，它关注的是预测的“准确性”。查全率，也称为“召回率”，衡量的是模型在所有实际为正的样本中，成功识别出的比例，它关注的是预测的“覆盖率”。查准率和查全率是一对矛盾的度量，一个偏高另一个往往偏低。

将查全率作为横坐标，查准率作为纵坐标，可以绘制`P-R`图，该图中 **查全率=查准率** 的点被称为平衡点(BEP)，可以根据BEP的取值来判断哪个学习器的效果更好。但BEP还是过于简化了些，更常用的是`F1`度量：

&lt;div style=&#34;text-align: center&#34;&gt;$F1 = \frac{2 \times  P \times  R}{P &#43; R} = \frac{2 \times  TP}{样本总数 &#43; TP  - TN}$&lt;/div&gt;

F1度量是查准率和查全率的调和平均数，提供了一种平衡两者的方式，特别适合在查准率和查全率都很重要的情况下使用。在一些应用中，对查准率和查全率的重视程度有所不同，这时就需要使用F1度量的一般形式——$F_\beta$：

&lt;div style=&#34;text-align: center&#34;&gt;$F_\beta = \frac{(1&#43;\beta)^2 \times P \times R}{(\beta^2 \times P) &#43; R} $&lt;/div&gt;

该形式可以表达出对查准率和查全率的不同偏好。$\beta &gt; 1$时查全率有更大影响；$\beta &lt; 1$时查准率有更大影响。 $\beta = 1$时退化为标准的F1。

#### 3. ROC和AUC

很多机器学习是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。这个实值的好坏，直接决定了学习器的泛化能力。ROC和AUC指标的优点是能够在不同的阈值下全面考察模型的表现，特别适合于不平衡数据集的评价。

&lt;br&gt;ROC曲线

* **定义**：ROC曲线是一条通过不同阈值下模型的真阳性率（True Positive Rate, TPR）和假阳性率（False Positive Rate, FPR）来绘制的曲线。
* **横轴（FPR）**：假阳性率，即所有实际为负的样本中被误判为正的比例。
  &lt;div style=&#34;text-align: center&#34;&gt;$FPR = \frac{FP}{FP&#43;TN} $&lt;/div&gt;
* **纵轴（TPR）**：真阳性率，即所有实际为正的样本中被正确预测为正的比例，也称为查全率或召回率。
  &lt;div style=&#34;text-align: center&#34;&gt;$TPR = \frac{TP}{TP&#43;FN} $&lt;/div&gt;

ROC曲线的理想情况是越靠近左上角越好，表示较高的真阳性率和较低的假阳性率。完美模型的ROC曲线会经过左上角点 **`(0,1)`**。

&lt;br&gt;AUC

* **定义**：AUC是ROC曲线下的面积，数值范围为0到1，表示模型对正负样本的区分能力。
  * **AUC = 1**：表示模型完全区分正负样本，性能极佳。
  * **AUC = 0.5**：表示模型没有任何区分能力，相当于随机猜测。
  * **AUC &lt; 0.5**：意味着模型表现反常，甚至不如随机猜测。

AUC值越高，表示模型越具有区分正负样本的能力。在实际应用中，AUC常用于评估模型的总体表现，特别适合不平衡数据集，因为它能综合考虑不同阈值下的表现。

#### 4. 代价敏感错误率与代价曲线

该性能度量特别适用于在不同类型错误有不同代价的场景中使用。例如，在金融、医疗等领域中，错误分类的代价可能不均衡，可模型评价需要考虑这些代价差异。

假设分类问题中有两类，即正类和负类。代价敏感错误率通过在混淆矩阵的各类错误上施加不同的权重来计算，如下表所示：

|              | 预测正类 (P) | 预测负类 (N) |
| ------------ | ------------ | ------------ |
| 实际正类 (P) | 0            | C(FN)        |
| 实际负类 (N) | C(FP)        | 0            |

其中：

* **C(FP)** 是将负类误判为正类的代价（假阳性代价）。
* **C(FN)** 是将正类误判为负类的代价（假阴性代价）。

**计算公式为**：

&lt;div style=&#34;text-align: center&#34;&gt;$代价敏感错误率 = \frac{\sum (误分类数 \times 该误分类的代价)}{总样本数}$&lt;/div&gt;

代价敏感错误率通过给不同的错误类型赋予不同的代价来评估模型的表现，使模型更关注代价较高的错误类型。

&lt;br&gt;**代价曲线**是展示模型在不同代价下性能的一种曲线图，可以直观反映模型在各种代价条件下的表现。代价曲线可以帮助我们判断模型在不同代价比例下的稳定性和性能。

代价曲线图的横轴是取值为[0, 1]的正例概率代价；纵轴是取值为[0, 1]的归一化代价。设ROC曲线上点的坐标为(TPR, FPR)，则可计算对应的FNR（假阴性率），然后再代价平面上绘制一条从（0，FPR）到（1，FNR的线段），该线段下的面积表示了该条件下的期望总体代价；如此将ROC曲线上的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围城的面积即为在所有条件下学习器的期望总体代价。

### 比较检验

有了实验评估方法和性能度量，看起来就能对学习器的性能进行评估比较了：先使用某个评估方法测得实验学习器的某个性能度量结果，然后对这些结果进行比较。但怎么进行比较呢？直接比大小吗？实际上，机器学习中性能比较是十分复杂的。

#### 1. 假设检验

#### 2. 交叉验证t检验

#### 3. McNemar检验

#### 4. Friedman检验与Nemenyi后续检验

### 偏差与方差

偏差-方差分解是解释学习算法泛化性能的一种重要工具。

## 强化学习


---

> 作者: [zyz](https://github.com/YouZhiZheng)  
> URL: http://localhost:1313/posts/80bdcb8/  

