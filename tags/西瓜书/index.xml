<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>西瓜书 - 标签 - zyz的技术博客</title>
    <link>http://localhost:1313/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/</link>
    <description>西瓜书 - 标签 | zyz的技术博客</description>
    <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>youzhizheng9@gmail.com (zyz)</managingEditor>
      <webMaster>youzhizheng9@gmail.com (zyz)</webMaster><lastBuildDate>Mon, 25 Nov 2024 11:14:23 &#43;0800</lastBuildDate><atom:link href="http://localhost:1313/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/" rel="self" type="application/rss+xml" /><item>
  <title>西瓜书学习笔记</title>
  <link>http://localhost:1313/posts/80bdcb8/</link>
  <pubDate>Mon, 25 Nov 2024 11:14:23 &#43;0800</pubDate>
  <author>zyz</author>
  <guid>http://localhost:1313/posts/80bdcb8/</guid>
  <description><![CDATA[第2章 模型评估与选择经验误差与过拟合错误率： 分类错误的样本占样本总数的比例。设m个样本中有a个样本分类错误，则错误率 $E = a / m$。
精度： 分类正确的样本占样本总数的比例。即 &ldquo;精度 = 1 - 错误率&rdquo;（精度一般用百分比表示）。
一般来说，将学习器的实际预测输出与样本的真实输出之间的差异称为 &ldquo;误差&rdquo;。
训练误差： 学习器在训练集上的误差（也叫经验误差）。
泛化误差： 学习器在新样本上的误差。
我们希望得到的学习器是能在新样本上有很好表现的学习器。为了能达到这个目的，应该从训练样本中尽可能学出适用于所有潜在样本的普遍规律，这样才能在遇到新样本时做出正确判断。然而，当学习器把训练样本学得太好时就会出现「过拟合」现象。与过拟合相对的是「欠拟合」。
过拟合： 学习器把训练样本自身的一些特点当作了所有潜在样本的特点，导致泛化性能下降。
欠拟合： 学习器对训练样本的一般性质尚未学好，没有很好地学习到训练样本中的模式和规律。
导致过拟合的因素有很多，其中最常见的情况是学习能力过于强大（学习能力由学习算法和数据内涵共同决定），以至于学习器把训练样本所包含的不太一般的特性都学到了。而欠拟合则通常是由于学习能力低下而造成的。过拟合是机器学习面临的关键障碍，是无法避免的，我们所能做的只是 &ldquo;缓解&rdquo; 过拟合。
在现实任务中，我们有许多学习算法可以选择，各类算法都必然带有一些针对过拟合的措施，甚至同于同一个算法来说，使用不同的参数配置时，也会产生不同的模型。那么该选择哪一个学习算法、使用哪一种参数配置呢？这就是机器学习中的「模型选择」问题。理想的解决方案为对候选模型的泛化误差进行评估，然后选择泛化误差最小的模型。然而，我们是无法直接获取泛化误差的，而训练误差又由于过拟合现象的存在不适合作为标准，那么在现实中如何进行模型的评估与选择呢？
评估方法我们通常会将数据集D划分为训练集S和测试集T，使用T来测试学习器对新样本的判别能力，然后以T上的测试误差作为泛化误差的近似。需要注意的是，T与S应该尽可能的 「互斥」。
1. 留出法该方法直接将数据集D划分为两个互斥的集合，其中一个作为训练集S，另一个作为测试集T，即$D=S \cup T, S \cap T=\varnothing$。使用测试误差来近似泛化误差。
需要注意的是，S 和 T 的划分要尽可能保持数据分布的一致性（即在不同数据集或数据样本中，数据的分布特性保持相同或相似的性质），避免因数据划分过程引入额外的偏差而对最终结果产生影响。
单次使用留出法得到的结果往往都是不够稳定和可靠的，因此，在使用留出法时，一般采用多若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。此外，我们希望评估的是用数据集D训练出来的模型的性能，但留出法需将D划分为S和T，这就会导致一个问题：若训练集包含绝大多数样本，则训练出的模型会更接近用D训练出的模型，但由于T比较小，评估结果可能不够稳定准确；若T多包含一些样本，则S与D的差别就会变大，导致被评估的模型与用D训练出的模型相比可能有较大差异，从而降低了评估结果的保真性。这个问题没有完美的解决方案，常见做法是将大约 2 / 3 ~ 4 / 5 的样本作为训练，剩下的作为测试。
2. 交叉验证法该方法将数据集D划分为 k 个大小相似的互斥子集，即$D=D_{1} \cup D_{2} \cup \ldots \cup D_{k}, D_{i} \cap D_{j}=\varnothing(i \neq j) \text {.]]></description>
</item>
</channel>
</rss>
